{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Classifier BoW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNfy_eV_HgnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3EMbsnlNc_n",
        "colab_type": "text"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "This is a problem to identify wheather a given news is fake or real.\n",
        "The Indepedent variables or factors used here can be title of news or news lines themselves.\n",
        "We will be looking at different model to solve this problem.\n",
        "In this notebook we will using Bag of Words model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrBM4Tt1N-fy",
        "colab_type": "text"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "The dataset for news is taken from Kaggle https://www.kaggle.com/c/fake-news/.\n",
        "The dataset is quite huge containing thousands of rows, we expect our model to be highly accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-u-Lbc5OFnS",
        "colab_type": "text"
      },
      "source": [
        "# Step 1 - Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2r8jY47OIHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aea36119-4f36-4717-94ed-48d07d9af263"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvzmxC4YOVr1",
        "colab_type": "text"
      },
      "source": [
        "# Step 2 - Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJATHQwOOX-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "d1ba9dfa-ab8c-461d-9f60-b278cdad58bf"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Data Science Projects/Dataset Fakenews.csv') # 1 is for fake 0 for real\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWm5Yk4yOpwd",
        "colab_type": "text"
      },
      "source": [
        "# Step 3 - Dealing with null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfllUVITOsCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2cc04c47-5162-4a9b-a2d9-90282c4b6b5e"
      },
      "source": [
        "print(data.isnull().sum())\n",
        "data.dropna(inplace = True)\n",
        "data.reset_index(inplace = True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id           0\n",
            "title      558\n",
            "author    1957\n",
            "text        39\n",
            "label        0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NaLW2GpOx2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "987ba4f3-a897-4f43-a579-571a608c0c80"
      },
      "source": [
        "print(data.isnull().sum())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index     0\n",
            "id        0\n",
            "title     0\n",
            "author    0\n",
            "text      0\n",
            "label     0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lljIJ2RNPAZO",
        "colab_type": "text"
      },
      "source": [
        "# Step 4 - Making dependent variable Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eeoIcywPE4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = data.iloc[:, -1]\n",
        "# We will be dealing with Independent Variable X later on"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns-5nEIvPH2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "dd8e76ae-1830-432d-fec9-1c235950bdfb"
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqm23lenPf8W",
        "colab_type": "text"
      },
      "source": [
        "# Step 5 - Cleaning the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akg7vMtdPj5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []                                           # Declaring a list\n",
        "for i in range(0,data.shape[0]):\n",
        "  review = re.sub('[^a-zA-Z]', ' ',  data['title'][i])  # Keeping only a-z or A-Z characters in news\n",
        "  review = review.lower()                               # Lower all characters\n",
        "  review = review.split()\n",
        "  ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words('english')            # Removing all stopwords/unnecessary words from news\n",
        "  all_stopwords.remove('not') #otherwise this word \"not\" will be included in stopwords removing of which from news is not idle\n",
        "  review = [ps.stem(word) for word in review if not word in set(all_stopwords) ]\n",
        "  review = ' '.join(review) #Converting back the review list to String\n",
        "  corpus.append(review)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWTcJ6_6QcVA",
        "colab_type": "text"
      },
      "source": [
        "# Step 6 - Creating Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzqEmA0iQfeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 5000, ngram_range = (1,3)) #take feature as combination of words \n",
        "X = cv.fit_transform(corpus).toarray()  # Finally independent variable X is ready"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVGFUQ3-Qpdd",
        "colab_type": "text"
      },
      "source": [
        "# Step 7 - Splitting dataset into Training and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njh5_-2rQsUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41rQlo4TQwx9",
        "colab_type": "text"
      },
      "source": [
        "# Step 8 - Creating Machine Learning models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pvOv6HyT87I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bda38018-45a1-4af3-9fcf-df923337a695"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, Y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyZqFD8CT_Gn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3dfae0d8-c7b3-4e19-c981-87743f273043"
      },
      "source": [
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "classifier = PassiveAggressiveClassifier()\n",
        "classifier.fit(X_train, Y_train)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
              "                            early_stopping=False, fit_intercept=True,\n",
              "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
              "                            n_jobs=None, random_state=None, shuffle=True,\n",
              "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
              "                            warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qrR18-HUA-k",
        "colab_type": "text"
      },
      "source": [
        "# Step 9 - Predicting results and checking accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rRvhiNSUHAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c26ca384-d454-4262-e5c9-a490dc3a49f8"
      },
      "source": [
        "yhat = classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(Y_test, yhat)\n",
        "accuracy = accuracy_score(Y_test, yhat)\n",
        "\n",
        "print(accuracy)\n",
        "print(cm)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9212469237079574\n",
            "[[1875  165]\n",
            " [ 123 1494]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PThv31hnYh4V",
        "colab_type": "text"
      },
      "source": [
        "# Step 10 - Checking which wards are most real and fake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUqvPq3HYlLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "8a6c6eb4-ef09-43ed-bbbd-2fc28010bb81"
      },
      "source": [
        "feature_names = cv.get_feature_names()\n",
        "coefficients = classifier.coef_[0]\n",
        "\n",
        "# Top 20 real news words\n",
        "sorted(zip(coefficients, feature_names), reverse = True)[:20]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3.8571537425702975, 'journal'),\n",
              " (3.7815966491552446, 'idiot'),\n",
              " (3.775170399164279, 'invad'),\n",
              " (3.581588917914471, 'gap'),\n",
              " (3.5745012060691366, 'trump need'),\n",
              " (3.429834890494482, 'american peopl'),\n",
              " (3.3814146979843276, 'humili'),\n",
              " (3.3047458357515245, 'hillari'),\n",
              " (3.301384952919329, 'poll show'),\n",
              " (3.1881157247178455, 'comment'),\n",
              " (3.1085406386443104, 'daesh'),\n",
              " (3.093216287037252, 'negoti'),\n",
              " (3.018388392428779, 'migrant crisi'),\n",
              " (2.893182234438597, 'report new york'),\n",
              " (2.862984274145243, 'jame matti'),\n",
              " (2.8318501875211615, 'quak'),\n",
              " (2.745629786078349, 'report new'),\n",
              " (2.687209547996219, 'gingrich'),\n",
              " (2.6783990977277856, 'trap'),\n",
              " (2.665046060322914, 'meddl')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci5jrpo_ZSoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "5a47fb76-6170-44d5-c17e-82f485316168"
      },
      "source": [
        "# Top 20 fake news words\n",
        "sorted(zip(coefficients, feature_names), reverse = False)[:20]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-7.271551256920901, 'breitbart'),\n",
              " (-4.3353405388308195, 'hillari clinton'),\n",
              " (-3.7972721545682884, 'penc'),\n",
              " (-3.7685837407901874, 'clinton aid'),\n",
              " (-3.7575113708312093, 'ross'),\n",
              " (-3.729416320091774, 'todd'),\n",
              " (-3.7248559551882026, 'virgil'),\n",
              " (-3.6953307726288314, 'delingpol'),\n",
              " (-3.5911267754762535, 'espn'),\n",
              " (-3.5212977714349507, 'streisand'),\n",
              " (-3.4618303179314287, 'cher'),\n",
              " (-3.442789691327568, 'macron'),\n",
              " (-3.377984925443203, 'town hall'),\n",
              " (-3.319626386785692, 'ck'),\n",
              " (-3.3068067021872736, 'abort'),\n",
              " (-3.2976365736937137, 'matti'),\n",
              " (-3.2208307687016626, 'gorka'),\n",
              " (-3.2026418384766835, 'new year'),\n",
              " (-3.2002125250061666, 'regist'),\n",
              " (-3.199840168963801, 'trump order')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0K0omkaZXDG",
        "colab_type": "text"
      },
      "source": [
        "# Step 11 - Saving model for future use and deplyoment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrG6eSfrZanv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as filename:\n",
        "  pickle.dump(classifier, filename)\n",
        "\n",
        "with open(\"cv.pkl\" , \"wb\") as filename:\n",
        "  pickle.dump(cv, filename)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB1vhTbzalmc",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "Lets compare accuracies by different models\n",
        "\n",
        "**Multinomial NB**\n",
        "\n",
        "Accuracy - 0.9004648619086683\n",
        "\n",
        "**Passive Agressive Classifier**\n",
        "\n",
        "0.9212469237079574\n",
        "\n",
        "Thus, it can be clearly seen that both the models Multinomial NB and Passive Agressive Classifier are great for NLP classification problems.\n",
        "Since Passive Agressive gave us a bit more accuracy , we will be chosing it for this specific problem.\n",
        "\n"
      ]
    }
  ]
}